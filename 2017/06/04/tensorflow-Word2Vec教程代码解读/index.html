<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="nlp,embedding," />





  <link rel="alternate" href="/atom.xml" title="回音壁" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="本文主要解读tensorflow在这个教程中的词嵌入(embedding)模块代码. 代码解读 教程中的word2vec.py是主要的词嵌入模块.word2vec_optimized.py是其优化版本,主要是将代价函数部分用C++实现了,提高了训练效率.这两个版本的模块都使用了C++实现的自定义读取操作,需要在运行前先编译出该操作的so库. 代码强制指定了各项计算都使用CPU,这是有道理的,见文末">
<meta name="keywords" content="nlp,embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow Word2Vec教程代码解读">
<meta property="og:url" content="http://yoursite.com/2017/06/04/tensorflow-Word2Vec教程代码解读/index.html">
<meta property="og:site_name" content="回音壁">
<meta property="og:description" content="本文主要解读tensorflow在这个教程中的词嵌入(embedding)模块代码. 代码解读 教程中的word2vec.py是主要的词嵌入模块.word2vec_optimized.py是其优化版本,主要是将代价函数部分用C++实现了,提高了训练效率.这两个版本的模块都使用了C++实现的自定义读取操作,需要在运行前先编译出该操作的so库. 代码强制指定了各项计算都使用CPU,这是有道理的,见文末">
<meta property="og:updated_time" content="2017-06-04T15:33:48.330Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tensorflow Word2Vec教程代码解读">
<meta name="twitter:description" content="本文主要解读tensorflow在这个教程中的词嵌入(embedding)模块代码. 代码解读 教程中的word2vec.py是主要的词嵌入模块.word2vec_optimized.py是其优化版本,主要是将代价函数部分用C++实现了,提高了训练效率.这两个版本的模块都使用了C++实现的自定义读取操作,需要在运行前先编译出该操作的so库. 代码强制指定了各项计算都使用CPU,这是有道理的,见文末">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/06/04/tensorflow-Word2Vec教程代码解读/"/>





  <title>tensorflow Word2Vec教程代码解读 | 回音壁</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?fa4e3c6f3b8d32a95de40c1781bed9f5";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>











  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">回音壁</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">念念不忘,必有回响</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/04/tensorflow-Word2Vec教程代码解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wang Henghuan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/me.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="回音壁">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">tensorflow Word2Vec教程代码解读</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-04T23:28:28+08:00">
                2017-06-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习与智能/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习与智能</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a class="cloud-tie-join-count" href="/2017/06/04/tensorflow-Word2Vec教程代码解读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count join-count" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要解读tensorflow在<a href="https://github.com/tensorflow/models/tree/master/tutorials/embedding" target="_blank" rel="external">这个教程</a>中的词嵌入(embedding)模块代码.</p>
<h3 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h3><ul>
<li>教程中的<code>word2vec.py</code>是主要的词嵌入模块.<code>word2vec_optimized.py</code>是其优化版本,主要是将代价函数部分用C++实现了,提高了训练效率.这两个版本的模块都使用了C++实现的自定义读取操作,需要在运行前先编译出该操作的so库.</li>
<li>代码强制指定了各项计算都使用CPU,这是有道理的,见文末的讨论.</li>
<li>解读以 <code>word2vec.py</code> 为主。</li>
</ul>
<h4 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h4><ul>
<li>使用<code>tf.app</code>来运行整个模块，以便使用<code>tf.app.flag</code>来接收和管理传入的参数。</li>
<li>接收的训练数据是连续的句子语料文本。使用C++自定义文本处理操作(也就是op),负责将连续的句子转换为符合skip模型的样本.</li>
<li><code>Class Options</code>用来管理配置和参数.</li>
<li><code>Class Word2Vec</code>是主体，包含主要逻辑。</li>
<li>使用<code>thread</code>库来进行多线程训练.</li>
</ul>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><h5 id="forward-self-examples-labels"><a href="#forward-self-examples-labels" class="headerlink" title="forward(self, examples, labels)"></a>forward(self, examples, labels)</h5><p>这个函数建立前向网络。输入的<code>examples</code>和<code>labels</code>分别是一个batch的训练样本和标签的tensor占位符,由预处理操作提供.函数返回输入给激活函数的logits.<br>首先初始化各权值矩阵.<code>emb</code>是我们需要的词嵌入字典,尺寸是<code>单词数</code> * <code>emb_dim</code>，然后逻辑回归网络仅有一层,其权值矩阵是<code>sm_w_t</code>(‘t’表示转置)和<code>sm_b</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Declare all variables we need.</span></div><div class="line">   <span class="comment"># Embedding: [vocab_size, emb_dim]</span></div><div class="line">   init_width = <span class="number">0.5</span> / opts.emb_dim</div><div class="line">   emb = tf.Variable(</div><div class="line">       tf.random_uniform(</div><div class="line">           [opts.vocab_size, opts.emb_dim], -init_width, init_width),</div><div class="line">       name=<span class="string">"emb"</span>)</div><div class="line">   self._emb = emb</div><div class="line"></div><div class="line">   <span class="comment"># Softmax weight: [vocab_size, emb_dim]. Transposed.</span></div><div class="line">   sm_w_t = tf.Variable(</div><div class="line">       tf.zeros([opts.vocab_size, opts.emb_dim]),</div><div class="line">       name=<span class="string">"sm_w_t"</span>)</div><div class="line"></div><div class="line">   <span class="comment"># Softmax bias: [emb_dim].</span></div><div class="line">   sm_b = tf.Variable(tf.zeros([opts.vocab_size]), name=<span class="string">"sm_b"</span>)</div></pre></td></tr></table></figure>
<p>注意这里<code>sm_w_t</code>是输出层权值矩阵.由于我们要输出的label数量等于单词表中单词的数量,所以这里矩阵的维度恰好是<code>单词数</code> * <code>emb_dim</code>,恰好与embedding矩阵的尺寸相同.<br>第一步是进行噪声采样，就是从所有词中随机选出若干词作为噪声。使用了<code>tf.nn.fixed_unigram_candidate_sampler</code>采样函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Nodes to compute the nce loss w/ candidate sampling.</span></div><div class="line">labels_matrix = tf.reshape(</div><div class="line">    tf.cast(labels,</div><div class="line">            dtype=tf.int64),</div><div class="line">    [opts.batch_size, <span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># Negative sampling.</span></div><div class="line">sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(</div><div class="line">    true_classes=labels_matrix,</div><div class="line">    num_true=<span class="number">1</span>,</div><div class="line">    num_sampled=opts.num_samples,</div><div class="line">    unique=<span class="keyword">True</span>,</div><div class="line">    range_max=opts.vocab_size,</div><div class="line">    distortion=<span class="number">0.75</span>,</div><div class="line">    unigrams=opts.vocab_counts.tolist()))</div></pre></td></tr></table></figure></p>
<p>然后，使用<code>tf.nn.embedding_lookup</code>函数从权值矩阵中选出对应的权值组成小的权值矩阵。这一步实际上是等效于以one-hot形式的输入数据乘以2层隐藏层1层输出层的权值矩阵。由于one-hot形式非常稀疏，我们仅选出第一层中对应的权值（其他权值在计算中都归零了）；同时由于我们仅关心真实词和采样噪声词的预测结果，所以仅选出第二层(输出层)中的对应权值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Embeddings for examples: [batch_size, emb_dim]</span></div><div class="line">example_emb = tf.nn.embedding_lookup(emb, examples)</div><div class="line"></div><div class="line"><span class="comment"># Weights for labels: [batch_size, emb_dim]</span></div><div class="line">true_w = tf.nn.embedding_lookup(sm_w_t, labels)</div><div class="line"><span class="comment"># Biases for labels: [batch_size, 1]</span></div><div class="line">true_b = tf.nn.embedding_lookup(sm_b, labels)</div><div class="line"></div><div class="line"><span class="comment"># Weights for sampled ids: [num_sampled, emb_dim]</span></div><div class="line">sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)</div><div class="line"><span class="comment"># Biases for sampled ids: [num_sampled, 1]</span></div><div class="line">sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)</div></pre></td></tr></table></figure></p>
<p>计算并返回logit：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># True logits: [batch_size, 1]</span></div><div class="line">true_logits = tf.reduce_sum(tf.multiply(example_emb, true_w), <span class="number">1</span>) + true_b</div><div class="line"></div><div class="line"><span class="comment"># Sampled logits: [batch_size, num_sampled]</span></div><div class="line"><span class="comment"># We replicate sampled noise labels for all examples in the batch</span></div><div class="line"><span class="comment"># using the matmul.</span></div><div class="line">sampled_b_vec = tf.reshape(sampled_b, [opts.num_samples])</div><div class="line">sampled_logits = tf.matmul(example_emb,</div><div class="line">                           sampled_w,</div><div class="line">                           transpose_b=<span class="keyword">True</span>) + sampled_b_vec</div><div class="line"><span class="keyword">return</span> true_logits, sampled_logits</div></pre></td></tr></table></figure></p>
<p>这里有个细节.注意到对于一个batch的样本而言,正确的词(标签)每个样本对应一个,一共有batch个,计算时对于每个样本而言是两个向量相乘(使用<code>tf.multiply</code>)再加上偏置.而对于抽到的负样本而言,一个batch的样本都使用同样的simple_num个干扰样本,对于每个样本的计算都是一个向量(样本的词向量)乘以一个矩阵(simple_num个噪声样本对应的权值向量拼接起来),使用<code>tf.matmul</code>.所以可以看到计算方式是不同的.</p>
<h5 id="nce-loss-self-true-logits-sampled-logits"><a href="#nce-loss-self-true-logits-sampled-logits" class="headerlink" title="nce_loss(self, true_logits, sampled_logits)"></a>nce_loss(self, true_logits, sampled_logits)</h5><p>训练中实际用到的代价函数.NCE代价函数简单来说就是把标签词的交叉熵与干扰词的交叉熵相加得到的结果.我们指定标签词应该得到1,干扰词应该得到0,这样当标签词的activation越接近1,交叉熵就越小;当干扰词的activation越接近0,其交叉熵也越小.求两者相加的NCE代价越小,就等价于使模型结果尽量倾向于标签词而远离干扰词.<br>代码首先使用<code>tf.nn.sigmoid_cross_entropy_with_logits</code>函数计算标签词和干扰词的交叉熵.该函数的<code>labels</code>参数是一个维度与输入<code>logits</code>相同的tensor,指定了对应位置上的logit应该被判断为1还是0：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cross-entropy(logits, labels)</span></div><div class="line">opts = self._options</div><div class="line">true_xent = tf.nn.sigmoid_cross_entropy_with_logits(</div><div class="line">    labels=tf.ones_like(true_logits), logits=true_logits)</div><div class="line">sampled_xent = tf.nn.sigmoid_cross_entropy_with_logits(</div><div class="line">    labels=tf.zeros_like(sampled_logits), logits=sampled_logits)</div></pre></td></tr></table></figure></p>
<p>然后,将标签词的交叉熵与噪音词的交叉熵相加,并且在<code>batch_size</code>上做平均,就得到了NCE:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># NCE-loss is the sum of the true and noise (sampled words)</span></div><div class="line"><span class="comment"># contributions, averaged over the batch.</span></div><div class="line">nce_loss_tensor = (tf.reduce_sum(true_xent) +</div><div class="line">                   tf.reduce_sum(sampled_xent)) / opts.batch_size</div><div class="line"><span class="keyword">return</span> nce_loss_tensor</div></pre></td></tr></table></figure>
<h5 id="build-graph-self"><a href="#build-graph-self" class="headerlink" title="build_graph(self)"></a>build_graph(self)</h5><p>建立训练网络.通过组合前向网络和优化器,得到最终的训练tensor.<br>首先,通过C++自定义的<code>word2vec.skipgram_word2vec</code>操作得到输入数据的tensor,这些样本将符合skipgram模型.关于skipgram模型的原理这里不再介绍,符合该模型的输入样本是一个词(id),其对应的标签也是一个词(id).每个样本表示出现该词时,其上下文(一个指定的窗口范围内)有可能出现如标签所标记的词.<br>这个操作将返回若干统计量和包含输入数据以及标签的占位符.这些返回参数都是tensor,需要通过<code>session.run</code>才能获取到结果值.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The training data. A text file.</span></div><div class="line">    <span class="comment">#处理数据读取过程.把文本按照skip模型的方式分割成一个个样本</span></div><div class="line">    <span class="comment">#这里返回的参数都是placeholder,需要通过session.run才能填充实际数据</span></div><div class="line">    <span class="comment">#word:包含所有单词的向量</span></div><div class="line">    <span class="comment">#counts:每个单词在语料中出现的频数</span></div><div class="line">    <span class="comment">#words_per_epoch:每个epoch中包含的单词数目</span></div><div class="line">    <span class="comment">#self._epoch:当前epoch序号</span></div><div class="line">    <span class="comment">#self._words:当前已处理单词数</span></div><div class="line">    <span class="comment">#examples:样本词id(一个batch大小)</span></div><div class="line">    <span class="comment">#labels:标签词id(一个batch大小)</span></div><div class="line">    (words, counts, words_per_epoch, self._epoch, self._words, examples,</div><div class="line">     labels) = word2vec.skipgram_word2vec(filename=opts.train_data,</div><div class="line">                                          batch_size=opts.batch_size,</div><div class="line">                                          window_size=opts.window_size,</div><div class="line">                                          min_count=opts.min_count,</div><div class="line">                                          subsample=opts.subsample)</div><div class="line">    <span class="comment">#获取单词表和频数统计</span></div><div class="line">    (opts.vocab_words, opts.vocab_counts,</div><div class="line">     opts.words_per_epoch) = self._session.run([words, counts, words_per_epoch])</div><div class="line">    opts.vocab_size = len(opts.vocab_words)</div><div class="line">    print(<span class="string">"Data file: "</span>, opts.train_data)</div><div class="line">    print(<span class="string">"Vocab size: "</span>, opts.vocab_size - <span class="number">1</span>, <span class="string">" + UNK"</span>)</div><div class="line">    print(<span class="string">"Words per epoch: "</span>, opts.words_per_epoch)</div><div class="line">    self._examples = examples</div><div class="line">    self._labels = labels</div><div class="line">    self._id2word = opts.vocab_words</div><div class="line">    <span class="comment">#反向查询表</span></div><div class="line">    <span class="keyword">for</span> i, w <span class="keyword">in</span> enumerate(self._id2word):</div><div class="line">      self._word2id[w] = i</div></pre></td></tr></table></figure>
<p>完成对语料的预处理后,将前向过程和代价函数组合起来,使用优化器自动进行反向过程.最后定义了一个记录所有tensor状态的<code>saver</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#前向计算标签和干扰项的logits</span></div><div class="line">true_logits, sampled_logits = self.forward(examples, labels)</div><div class="line"><span class="comment">#根据上述logits计算nce</span></div><div class="line">loss = self.nce_loss(true_logits, sampled_logits)</div><div class="line">tf.summary.scalar(<span class="string">"NCE loss"</span>, loss)</div><div class="line">self._loss = loss</div><div class="line"><span class="comment">#修正权重</span></div><div class="line">self.optimize(loss)</div><div class="line"></div><div class="line"><span class="comment"># Properly initialize all variables.</span></div><div class="line">tf.global_variables_initializer().run()</div><div class="line"></div><div class="line">self.saver = tf.train.Saver()</div></pre></td></tr></table></figure>
<h5 id="train-self"><a href="#train-self" class="headerlink" title="train(self)"></a>train(self)</h5><p>实际执行训练过程.这里使用了<code>thread</code>模块来进行多线程训练.代码仅进行了一次epoch就停机了,实际使用时增加epoch数目也许可以获得更好的结果.<br>真正执行训练的代码在子线程函数中:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_thread_body</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment">#获取当前epoch序号</span></div><div class="line">    initial_epoch, = self._session.run([self._epoch])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">      <span class="comment">#每次执行都是执行一个batch.当处理完一次epoch的单词之后,就退出这个线程</span></div><div class="line">      _, epoch = self._session.run([self._train, self._epoch])</div><div class="line">      <span class="keyword">if</span> epoch != initial_epoch:</div><div class="line">        <span class="keyword">break</span></div></pre></td></tr></table></figure></p>
<p>这个函数首先run一次<code>epoch</code>tensor获取当前处理的epoch.之后通过<code>session.run</code>计算<code>train</code>tensor,这就进行了前向和反向操作,同时也获取了新的<code>epoch</code>值.当这个值与初始epoch序号不同时,就终止了训练.<br>这个子线程通过<code>thread</code>库进行了调用:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">workers = []</div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> xrange(opts.concurrent_steps):</div><div class="line">  t = threading.Thread(target=self._train_thread_body)</div><div class="line">  t.start()</div><div class="line">  workers.append(t)</div></pre></td></tr></table></figure>
<p>训练线程启动之后,代码进入了一个循环,将不断run那些统计tensor以打印当前的状态,并定时保存checkpoint.在<code>epoch</code>的值增加之后(跑完了一遍epoch)就等待各线程结束:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#进行循环统计信息</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  time.sleep(opts.statistics_interval)  <span class="comment"># Reports our progress once a while.</span></div><div class="line">  <span class="comment">#算一次前向过程以提供统计数据</span></div><div class="line">  (epoch, step, loss, words, lr) = self._session.run(</div><div class="line">      [self._epoch, self.global_step, self._loss, self._words, self._lr])</div><div class="line">  now = time.time()</div><div class="line">  <span class="comment">#计算相关统计参数并打印</span></div><div class="line">  last_words, last_time, rate = words, now, (words - last_words) / (</div><div class="line">      now - last_time)</div><div class="line">  print(<span class="string">"Epoch %4d Step %8d: lr = %5.3f loss = %6.2f words/sec = %8.0f\r"</span> %</div><div class="line">        (epoch, step, lr, loss, rate), end=<span class="string">""</span>)</div><div class="line">  sys.stdout.flush()</div><div class="line">  <span class="comment">#记录信息</span></div><div class="line">  <span class="keyword">if</span> now - last_summary_time &gt; opts.summary_interval:</div><div class="line">    summary_str = self._session.run(summary_op)</div><div class="line">    summary_writer.add_summary(summary_str, step)</div><div class="line">    last_summary_time = now</div><div class="line">  <span class="comment">#保存checkpoint</span></div><div class="line">  <span class="keyword">if</span> now - last_checkpoint_time &gt; opts.checkpoint_interval:</div><div class="line">    self.saver.save(self._session,</div><div class="line">                    os.path.join(opts.save_path, <span class="string">"model.ckpt"</span>),</div><div class="line">                    global_step=step.astype(int))</div><div class="line">    last_checkpoint_time = now</div><div class="line">  <span class="comment">#处理完整一个epoch之后就退出统计,并且等待各计算线程工作完毕</span></div><div class="line">  <span class="keyword">if</span> epoch != initial_epoch:</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure>
<h3 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h3><ul>
<li>在我的机器上，默认代码虽然占用了gpu显存但是并不会真正利用gpu。强行修改代码利用gpu的话，速度比cpu还慢。待解决。</li>
<li>目前看来，代码使用cpu(默认)就是比gpu快速。打印tensor分配日志可以看到部分op被分配到了cpu上。实际上，自定义实现的op都没有gpu版本，这包括了数据IO操作，和optimized版本代码中的损失函数操作。由于embedding可能很大，所以这个矩阵不应该被分配到gpu上；同时，embedding lookup和embedding update都不会被分配到gpu上。</li>
<li>目前观察到现象是，无论是<code>word2vec.py</code>还是<code>word2vec_optimized.py</code>，都是指定分配任务到cpu上的性能更好，而如果不强制指定cpu，那么<code>word2vec_optimized.py</code>性能没有太大变化，而<code>word2vec.py</code>性能严重下降。两者的差异在于，<code>word2vec_optimized.py</code>自行实现了nce损失函数的op（没有gpu代码）。推测出现性能差异的原因是，<code>word2vec.py</code>的 embedding update 操作在cpu上而损失函数（以及隐藏层更新）在gpu上，这种操作带来了严重的通信开销；而<code>word2vec_optimized.py</code>的损失函数也在cpu上（因为自定义op没有实现gpu版本），所以损失不大。也可以指定仅将matmul操作分配到gpu上，实测性能好于让机器自行分配，但是仍然比指定全cpu性能差。</li>
<li>由此可以得出，word embeding最好在cpu上做。</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/embedding/" rel="tag"># embedding</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/23/深度学习在文本分类中的应用/" rel="next" title="深度学习在文本分类中的应用">
                <i class="fa fa-chevron-left"></i> 深度学习在文本分类中的应用
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/04/词向量-词嵌入-学习笔记/" rel="prev" title="词向量(词嵌入)学习笔记">
                词向量(词嵌入)学习笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/me.png"
               alt="Wang Henghuan" />
          <p class="site-author-name" itemprop="name">Wang Henghuan</p>
           
              <p class="site-description motion-element" itemprop="description">一些与深度学习相关的笔记</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码解读"><span class="nav-number">1.</span> <span class="nav-text">代码解读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#overview"><span class="nav-number">1.1.</span> <span class="nav-text">overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练"><span class="nav-number">1.2.</span> <span class="nav-text">训练</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#forward-self-examples-labels"><span class="nav-number">1.2.1.</span> <span class="nav-text">forward(self, examples, labels)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#nce-loss-self-true-logits-sampled-logits"><span class="nav-number">1.2.2.</span> <span class="nav-text">nce_loss(self, true_logits, sampled_logits)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#build-graph-self"><span class="nav-number">1.2.3.</span> <span class="nav-text">build_graph(self)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#train-self"><span class="nav-number">1.2.4.</span> <span class="nav-text">train(self)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一些问题"><span class="nav-number">2.</span> <span class="nav-text">一些问题</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Henghuan</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "0eed3b875dc14040bb040cb04763ce18",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (search_path.endsWith("json")) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

</body>
</html>
